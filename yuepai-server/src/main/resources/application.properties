spring.application.name = yuepai-server
dubbo.application.name =  yuepai-server
dubbo.protocol.name = dubbo
dubbo.protocol.port = 20882
dubbo.registry.address = zookeeper://localhost:2181
#数据源配置文件
#本地数据库
# 主数据源，默认的
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource 
spring.datasource.url = jdbc:mysql://localhost:3306/yuepai?useUnicode=true&characterEncoding=utf8&serverTimezone=UTC
#&serverTimezone=Asia/Shanghai
spring.datasource.username = root
spring.datasource.password =  root
spring.datasource.driverClassName = com.mysql.jdbc.Driver
spring.datasource.filters = stat,wall,log4j
# 下面为连接池的补充设置，应用到上面所有数据源中
# 初始化大小，最小，最大，超时时间
spring.datasource.dbcp2.min-idle=1 
spring.datasource.dbcp2.initial-size=1
spring.datasource.dbcp2.max-total=2
spring.datasource.dbcp2.max-wait-millis=200
#数据库配置结束
#redis数据库配置开始
#spring.redis.host=127.0.0.1
redis.host=127.0.0.1
## Redis服务器连接端口
redis.port=6379
## 连接超时时间（毫秒）
redis.timeout=3
## Redis服务器连接密码（默认为空）
redis.password=123456
## 连接池中的最大连接数
redis.poolMaxTotal=10
## 连接池中的最大空闲连接
redis.poolMaxIdle=10
## 连接池最大阻塞等待时间（使用负值表示没有限制）
redis.poolMaxWait=3
#redis数据库配置结束

#页面热加载
spring.thymeleaf.cache = true
#不返回为null的字段
spring.jackson.default-property-inclusion=non_null

logging.level.org.springframework=WARN
logging.level.org.spring.springboot.dao=DEBUG
logging.file=logs/spring-boot-logging.log
#在控制台输出sql，level后面的是我mapper的包名
logging.level.com.yuepai.yuepaiserver.mapper=debug

#端口号
server.port = 8111
#富文本图片上传
#web.upload-path=D:/test/image/
web.upload-path=C:/tomcat9/webapps/image
spring.mvc.static-path-pattern=/**
spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,\
  classpath:/static/,classpath:/public/,file:${web.upload-path},file:static/,C:/tomcat9/webapps/image/

#kafka地址 brokers集群地址用,隔开
spring.kafka.bootstrap-servers=127.0.0.1:9092
#消费者的配置
##Kafka中没有初始偏移或如果当前偏移在服务器上不再存在时,默认区最新 ，有三个选项 【latest, earliest, none】
spring.kafka.consumer.auto-offset-reset=latest
#是否开启自动提交
spring.kafka.consumer.enable-auto-commit=true
#自动提交的时间间隔
spring.kafka.consumer.auto-commit-interval=100
#key的解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#value的解码方式
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#在kafka/config文件的consumer.properties中有配置
spring.kafka.consumer.group-id=test-consumer-group
